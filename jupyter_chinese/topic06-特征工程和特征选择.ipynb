{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征工程和特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次实验通过特征提取、特征转换、特征选择三个过程介绍数据预处理方法，特征提取将原始数据转换为适合建模的特征，特征转换将数据进行变换以提高算法的准确性，特征选择用来删除无用的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 知识点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 特征提取\n",
    "- 特征转换\n",
    "- 特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次实验的一些示例将使用 Renthop 公司的数据集。首先载入数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载数据并解压\n",
    "!wget -nc \"../../data/renthop_train.json.gz\"\n",
    "!gunzip \"renthop_train.json.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:07.067528Z",
     "start_time": "2018-03-15T14:06:02.181930Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:07.067528Z",
     "start_time": "2018-03-15T14:06:02.181930Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('renthop_train.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实践中，很少有数据是以矩阵形式保存、可以直接使用的，这就是为什么需要对数据进行特征提取。首先看看在一些常见的数据类型中特征提取是如何进行的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本的处理方法非常多，本次实验介绍最流行的一种处理过程："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在处理文本之前，必须对文本进行切分词（tokenzie）操作，也就是将文本切分为单元（token）。在最简单的情形下，一个 token 就是一个单词。但直接按单词切分可能会损失一些信息，比如「Santa Barbara」应该是一个整体，却被切分为 2 个 token。现成的分词器（tokenizer）会考虑语言的特性，但也会出错，特别是当你处理特定来源的文本时（报纸、俚语、误拼、笔误）。\n",
    "- 接下来需要正则化数据。对文本而言，这涉及词干提取（stemming）和词形还原（lemmatization）方法。这两个方法是词形规范化的两类重要方式，都能够达到有效归并词形的目的，二者既有联系也有区别。两者的区别可以参考《Introduction to Information Retrieval》一书的 [<i class=\"fa fa-external-link-square\" aria-hidden=\"true\"> Stemming and lemmatization</i>](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) 一节。\n",
    "- 当文档被转换为单词序列之后，就可以用向量表示它。最简单的方法是词袋（Bag of Words）模型：创建一个长度等于字典的向量，计算每个单词出现在文本中的次数，然后将次数放入向量对应的位置中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面用代码来表述词袋模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:07.087385Z",
     "start_time": "2018-03-15T14:06:07.068964Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "texts = ['i have a cat',\n",
    "         'you have a dog',\n",
    "         'you and i have a cat and a dog']\n",
    "\n",
    "vocabulary = list(enumerate(set([word for sentence\n",
    "                                 in texts for word in sentence.split()])))\n",
    "print('Vocabulary:', vocabulary)\n",
    "\n",
    "\n",
    "def vectorize(text):\n",
    "    vector = np.zeros(len(vocabulary))\n",
    "    for i, word in vocabulary:\n",
    "        num = 0\n",
    "        for w in text:\n",
    "            if w == word:\n",
    "                num += 1\n",
    "        if num:\n",
    "            vector[i] = num\n",
    "    return vector\n",
    "\n",
    "\n",
    "print('Vectors:')\n",
    "for sentence in texts:\n",
    "    print(vectorize(sentence.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图是一个简化的词袋模型实现过程，在实际应用中，还需要考虑停止词，字典的最大长度等问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=50% src='https://doc.shiyanlou.com/courses/uid214893-20190505-1557032863965'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当使用词袋模型时，文本中的单词顺序信息会丢失，这意味着向量化之后，“i have no cows”（我没有牛）和“no, i have cows”（没，我有牛）会变得一样，尽管事实上它们的意思截然相反。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了避免这个问题，可以转而使用 N-Gram 模型。下面载入相关库，建立 N-Gram 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:08.998673Z",
     "start_time": "2018-03-15T14:06:07.088376Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1, 1))\n",
    "vect.fit_transform(['no i have cows', 'i have no cows']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:09.002804Z",
     "start_time": "2018-03-15T14:06:08.999801Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改变参数，再次建立 N-Gram 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:09.110448Z",
     "start_time": "2018-03-15T14:06:09.003924Z"
    }
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "vect.fit_transform(['no i have cows', 'i have no cows']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:09.218867Z",
     "start_time": "2018-03-15T14:06:09.113204Z"
    }
   },
   "outputs": [],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T14:13:25.767656Z",
     "start_time": "2018-03-14T14:13:25.763924Z"
    }
   },
   "source": [
    "除了基于单词生成 N-Gram 模型，在某些情形下，可以基于字符生成 N-Gram 模型，以考虑相关单词的相似性或笔误，下面基于字符生成 N-Gram 模型，并查看它们之间的欧氏距离。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:09.774148Z",
     "start_time": "2018-03-15T14:06:09.220060Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(3, 3), analyzer='char_wb')\n",
    "\n",
    "n1, n2, n3, n4 = vect.fit_transform(\n",
    "    ['andersen', 'petersen', 'petrov', 'smith']).toarray()\n",
    "\n",
    "euclidean(n1, n2), euclidean(n2, n3), euclidean(n3, n4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有时候，在语料库（数据集的全部文档）中罕见但在当前文本中出现的专业词汇可能会非常重要。因此，通过增加专业词汇的权重把它们和常用词区分开，是很合理的，这一方法称为 TF-IDF（词频-逆向文档频率），其默认选项为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ idf(t,D) = \\log\\frac{\\mid D\\mid}{df(d,t)+1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ tfidf(t,d,D) = tf(t,d) \\times idf(t,D) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用上述这些传统方法及模型，在简单问题上就可能得到不错的效果，可以作为后续模型的基线。然而，倘若不喜欢前面的传统方法，还有 Word2Vec、GloVe、Fasttext 等方法可以选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec 是词嵌入算法的一个特殊情形。使用 Word2Vec 和类似的模型，不仅可以向量化高维空间（通常是成百上千维的空间）中的单词，还能比较它们的语义相似度。下图演示了一个经典例子：king（王）- man（男）+ woman（女） = queen（皇后）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://doc.shiyanlou.com/courses/uid214893-20190505-1557032917121)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得注意的是，Word2Vec 模型并不理解单词的意思，只是尝试将用于相同的上下文的单词向量放在相近的位置。这样的模型需要在非常大的数据集上训练，使向量坐标能够捕捉语义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 图像数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理图像既简单又复杂。简单是因为现在我们可以直接使用某个流行的预训练网络，而不用思考太多；复杂是因为，如果深入图像领域，那么就需要考虑非常多的细节问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 GPU 不够强的年代，图像的特征提取本身是一个非常复杂的领域。我们需要对较低的层进行提取，检测角点、区域边界、色彩分布统计等。如果你对这些经典方法感兴趣，可以看下 skimage 和 SimpleCV 这两个库。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，和图像相关的问题，常常可以直接使用卷积神经网络来完成。你不需要从头设计网络架构，从头训练网络，下载一个当前最先进的预训练网络及其权重，“分离”网络的最后一个全连接层，增加针对特定任务的新层，接着在新数据上训练网络即可。这一让预训练网络适应特定任务的过程被称为微调（fine-tuning）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图是 fine-tuning 的一个例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=60% src='https://doc.shiyanlou.com/courses/uid214893-20190505-1557032976201'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果任务仅仅是对图像进行向量化（抽取图像的特征），那么只需移除最后一层，使用前一层的输出即可，下面通过调用 Keras 中的预训练网络进行图像特征抽取。首先加载相关库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:25.714680Z",
     "start_time": "2018-03-15T14:06:09.775547Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from scipy.misc import face\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时，由于原 ResNet50 模型托管在外网上，速度较慢。我们直接通过实验楼服务器来下载到线上环境指定路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接运行下载预训练模型，本地练习时无需本行代码，直接去掉即可\n",
    "!wget - P \"/root/.keras/models\" - nc \"http://labfile.oss.aliyuncs.com/courses/1283/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载 Keras 中的预训练网络 ResNet50，且移除最后一层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:25.714680Z",
     "start_time": "2018-03-15T14:06:09.775547Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resnet_settings = {'include_top': False, 'weights': 'imagenet'}\n",
    "resnet = ResNet50(**resnet_settings)\n",
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:25.714680Z",
     "start_time": "2018-03-15T14:06:09.775547Z"
    }
   },
   "source": [
    "查看所输入的图片，这是一张浣熊图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:25.714680Z",
     "start_time": "2018-03-15T14:06:09.775547Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = image.array_to_img(face())\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:27.770041Z",
     "start_time": "2018-03-15T14:06:25.718729Z"
    }
   },
   "source": [
    "通过 `resise()` 方法将图像尺寸调整为（224，224）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:27.770041Z",
     "start_time": "2018-03-15T14:06:25.718729Z"
    }
   },
   "outputs": [],
   "source": [
    "img = img.resize((224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:27.770041Z",
     "start_time": "2018-03-15T14:06:25.718729Z"
    }
   },
   "source": [
    "通过 `expand_dims()` 方法添加一个额外的维度，以适配网络模型的输入格式，其输入格式为张量形状（batch_size, width, height, n_channels）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:27.770041Z",
     "start_time": "2018-03-15T14:06:25.718729Z"
    }
   },
   "outputs": [],
   "source": [
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "features = resnet.predict(x)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的结果就是抽取的浣熊图像特征矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T14:44:25.102755Z",
     "start_time": "2018-03-14T14:44:24.374869Z"
    }
   },
   "source": [
    "如果想识别图像上的文字，无需使用复杂的神经网络，使用 pytesseract 库即可完成。首先载入相关库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytesseract  # 安装必要的模块\n",
    "!apt install tesseract-ocr -y\n",
    "!apt install libtesseract-dev -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T14:47:46.671934Z",
     "start_time": "2018-03-14T14:47:43.945326Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T14:47:46.671934Z",
     "start_time": "2018-03-14T14:47:43.945326Z"
    }
   },
   "source": [
    "搜索一张图片并使用 pytesseract 库进行识别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T14:47:46.671934Z",
     "start_time": "2018-03-14T14:47:43.945326Z"
    }
   },
   "outputs": [],
   "source": [
    "img = 'https://doc.shiyanlou.com/courses/uid214893-20190430-1556614744119'\n",
    "img = requests.get(img)\n",
    "img = Image.open(BytesIO(img.content))\n",
    "text = pytesseract.image_to_string(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T14:47:46.671934Z",
     "start_time": "2018-03-14T14:47:43.945326Z"
    }
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，pytesseract 并不能识别所有图片上的文字。比如，下面一张源自 Renthop 的图片，仅能识别其上的几个单词，还有大部分文字无法识别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = requests.get(\n",
    "    'https://doc.shiyanlou.com/courses/uid214893-20190505-1557033014576')\n",
    "img = Image.open(BytesIO(img.content))\n",
    "pytesseract.image_to_string(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 地理空间数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "地理空间数据相对文本和图像而言并不那么常见，但掌握处理地理空间数据的基本技术仍然是有用的，并且这一领域中有很多相当成熟的解决方案可以直接使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "地理空间数据常常以地址或坐标（经纬度）的形式保存。根据任务需求的不同，可能需要用到两种互逆的操作：地理编码（由地址重建坐标点）和逆地理编码（由坐标点重建地址）。在实际项目中，这两个操作都可以通过访问外部 API（谷歌地图或 OpenStreetMap）来使用。不同的地理编码器各有其特性，不同地区的编码质量也不一样。幸运的是，GeoPy 之类的通用库封装了这些操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果有大量数据，那么很快便会达到外部 API 的限制。此外，从 HTTP 获取信息并不总是最快的方案。因此，有必要考虑使用本地版的 OpenStreetMap。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果只有少量的数据，并且不想使用一些新奇的功能，就可以使用 `reverse_geocoder` 来代替 OpenStreetMap，代码如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install reverse_geocoder  # 安装必要模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T15:12:50.468269Z",
     "start_time": "2018-03-14T15:12:50.455393Z"
    }
   },
   "outputs": [],
   "source": [
    "import reverse_geocoder as revgc\n",
    "revgc.search((df.latitude.iloc[0], df.longitude.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理地理编码时，别忘了地址可能包含错误值，因此需要清洗数据。坐标的错误值较少，但由于 GPS 噪声或特定地点（比如隧道、商业区）的精确度较低，可能导致坐标不准确。如果数据源是移动设备，地理位置可能不是由 GPS 而是由该区域的 WiFi 网络决定的，这会导致出现空间和远距离传送的 Bug，例如，当你经过曼哈顿时，可能会突然碰到芝加哥的 WiFi 地点，出现错误的坐标信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">WiFi 的位置追踪功能基于 SSID 和 MAC 地址，同一 SSID 和 MAC 地址可能对应不同的地点，例如，联合供应商标准化 MAC 地址路由，并将其投放于不同城市。甚至一家公司带着路由器搬迁到另一个办公地点都可能造成坐标信息的错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "地点周边常常有许多基础设施。因此，可以充分发挥想象力，基于生活经验和领域知识自己构建几个特征，如「地点到地铁口的距离」、「建筑物中的商户数」、「到最近的商店的距离」、「周边的 ATM 数目」等。对每个任务都可以很容易的想到几十个特征并从不同的外部资源中获取它们。当地点位于城市以外时，可以考虑从更加专业的数据源获取到的特征，比如海拔。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果两个以上的地点相互连接，可能有必要基于地点之间的路由提取特征。比如，距离（直线距离和基于路由图计算得出的道路距离），转弯数（包括左转和右转的比例），红路灯数，交叉路口数，桥梁数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 日期和时间数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可能认为日期和时间是标准化的，因为它们是如此普遍，不过，其中仍然有一些坑。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有些任务可能需要额外的日历特征。比如，现金提取可能与「账单日」相关联；地铁月卡的购买可能和「月初」相关联。一般而言，处理时序数据时，最好有一份包含公众节假日、异常天气情况及其他重要事件的日历。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理小时和分钟不像看起来那么简单。如果你将小时作为实数变量，那么 0<23 ，但实际上 0:00:00 02.01>23:00:00 01.01。如果将它们编码为类别变量，那么会生成大量特征，同时丢失时间接近度的信息，即 22 和 23 之间的差异将与 22 和 7 之间的差异相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以，我们可以使用一些更加复杂的方法来处理这些数据，比如将时间投影到圆上，然后使用其 cos 和 sin 值作为两个坐标的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:27.782320Z",
     "start_time": "2018-03-15T14:06:27.772449Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_harmonic_features(value, period=24):\n",
    "    value *= 2 * np.pi / period\n",
    "    return np.cos(value), np.sin(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一转换保留了时间点间的距离，对于需要估计距离的算法（kNN、SVM、k-均值等）使用这种方法来处理时间数据会很有帮助。下面计算经这种方法转化后 23 点和 1 点的距离。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:27.883311Z",
     "start_time": "2018-03-15T14:06:27.784833Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "euclidean(make_harmonic_features(23), make_harmonic_features(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算 9 点和 11 点的距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:28.250852Z",
     "start_time": "2018-03-15T14:06:27.884753Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "euclidean(make_harmonic_features(9), make_harmonic_features(11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算 9 点和 22 点的距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:28.801865Z",
     "start_time": "2018-03-15T14:06:28.252109Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "euclidean(make_harmonic_features(9), make_harmonic_features(22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面结果可知，它们之间的距离各不相同，距离信息被保留。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web 数据通常有用户的 User Agent 信息，这个信息非常重要，首先，从中可以提取操作系统信息。其次，可以据此创建 「is_mobile」（是否是移动端）特征。最后，可以通过它查看到浏览器类别。下面看看是如何操作的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，加载相关库和数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install user_agents  # 安装所需模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.336832Z",
     "start_time": "2018-03-15T14:06:28.804134Z"
    }
   },
   "outputs": [],
   "source": [
    "import user_agents\n",
    "\n",
    "ua = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/56.0.2924.76 Chrome/56.0.2924.76 Safari/537.36'\n",
    "ua = user_agents.parse(ua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.336832Z",
     "start_time": "2018-03-15T14:06:28.804134Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Is a bot? ', ua.is_bot)\n",
    "print('Is mobile? ', ua.is_mobile)\n",
    "print('Is PC? ', ua.is_pc)\n",
    "print('OS Family: ', ua.os.family)\n",
    "print('OS Version: ', ua.os.version)\n",
    "print('Browser Family: ', ua.browser.family)\n",
    "print('Browser Version: ', ua.browser.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T16:11:22.538721Z",
     "start_time": "2018-03-14T16:11:22.534198Z"
    }
   },
   "source": [
    "除了操作系统和浏览器外，你还可以查看 referrer、Accept-Language 和其他元信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T16:11:22.538721Z",
     "start_time": "2018-03-14T16:11:22.534198Z"
    }
   },
   "source": [
    "另一个有用的特征是 IP 地址，基于该数据可以提取出国家，乃至城市、网络运营商、连接类型（是否为移动网络）等信息。当然，该特征可能由于代理和数据库过期导致提取出的信息不准确，出现噪声。网络管理专家可能会尝试提取出更专业的信息，比如是否使用 VPN。另外，IP 地址数据和 Accept-Languag 是一对优秀的组合，它们可以互相印证：如果用户的 IP 地址显示在智利，而浏览器的本地化设为 ru_RU（俄罗斯），那么该用户的所在地就不清楚了，我们需要查看对应的特征栏 「is_traveler_or_proxy_user」（是旅行者还是代理用户）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对有些算法而言，特征转换是关键，而对另一些算法而言，该转换则毫无帮助。不是所有人都有能力或者愿意去进行特征转换，这是为什么决策树及其变体（随机森林、梯度提升）变得更加流行，因为它们在异常分布上的鲁棒性很好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 标准化和分布变换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最简单的转换是标准缩放（Standard Scaling），又称 Z 值标准化（Z-score normalization）："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ z= \\frac{x-\\mu}{\\sigma} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，严格意义上说，标准缩放并不生成正态分布，下面验证这一结论，首先载入相关库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.382748Z",
     "start_time": "2018-03-15T14:06:29.338320Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import shapiro\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.382748Z",
     "start_time": "2018-03-15T14:06:29.338320Z"
    }
   },
   "source": [
    "使用夏皮罗-威尔克检验（Shapiro–Wilk test）查看数据是否符合正态分布，首先输出原数据的检测统计量和假设检验所得的 p 值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.382748Z",
     "start_time": "2018-03-15T14:06:29.338320Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = beta(1, 10).rvs(1000).reshape(-1, 1)\n",
    "shapiro(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后输出经标准缩放处理后数据的检测统计量和假设检验所得的 p 值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.509590Z",
     "start_time": "2018-03-15T14:06:29.385020Z"
    }
   },
   "outputs": [],
   "source": [
    "shapiro(StandardScaler().fit_transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.509590Z",
     "start_time": "2018-03-15T14:06:29.385020Z"
    }
   },
   "source": [
    "夏皮罗-威尔克检验计算所得的 p 值说明原数据和经标准缩放处理后的数据都不符合正态分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不过，某种程度上而言，标准缩放能为离散值提供一些帮助。下面使用 `StandarScaler()` 方法对离散值进行标准缩放。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.602528Z",
     "start_time": "2018-03-15T14:06:29.511150Z"
    }
   },
   "outputs": [],
   "source": [
    "data = np.array([1, 1, 0, -1, 2, 1, 2, 3, -2, 4, 100]\n",
    "                ).reshape(-1, 1).astype(np.float64)\n",
    "StandardScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以直接用公式进行标准缩放。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.713603Z",
     "start_time": "2018-03-15T14:06:29.605288Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(data - data.mean()) / data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一个相当流行的选项是极小极大缩放（MinMax Scaling），将所有数据点纳入一个预先规定的区间（通常是 [0,1]）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ X_{norm}=\\frac{X-X_{min}}{X_{max}-X_{min}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面用 `MinMaxScaler()` 方法进行极小极大缩放。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.855619Z",
     "start_time": "2018-03-15T14:06:29.716000Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "MinMaxScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以直接用公式进行极小极大缩放。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:29.955155Z",
     "start_time": "2018-03-15T14:06:29.857042Z"
    }
   },
   "outputs": [],
   "source": [
    "(data - data.min()) / (data.max() - data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准缩放和极小极大缩放的应用类似，常常可以互相替换。然而，如果算法涉及计算数据点或向量之间的距离，默认的选项是标准缩放。而在可视化时，极小极大缩放很有用，因为它可以将特征纳入 [0,255] 区间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果某些数据并非正态分布，但可以由 [<i class=\"fa fa-external-link-square\" aria-hidden=\"true\"> 对数正态分布</i>](https://en.wikipedia.org/wiki/Log-normal_distribution) 刻画，那么这些数据很容易就能转换为正态分布，下面生成可以用对数正态分布来刻画的数据，并查看其是否符合正态分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:30.067680Z",
     "start_time": "2018-03-15T14:06:29.957011Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import lognorm\n",
    "\n",
    "data = lognorm(s=1).rvs(1000)\n",
    "shapiro(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面结果表明生成的原数据不符合正态分布。下面将它进行转换，查看转换后的数据是否符合正态分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:30.180348Z",
     "start_time": "2018-03-15T14:06:30.069180Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shapiro(np.log(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述结果表明，转换后的数据符合正态分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数正态分布适用于描述薪水、安保费用、城区人口、网络文章评论等信息。然而，底层分布不一定非得是对数正态分布才能应用这一过程，它可以被应用于任何右重尾的分布。此外，还可以尝试使用其他类似的转换，这类转换的例子包括 Box-Cox 转换（对数转换是 Box-Cox 转换的一个特例）和 Yeo-Johnson 转换（将应用范围扩展至负数）。此外，也可以尝试在特征上加上一个常量：`np.log (x + const)`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上述例子中处理的都是合成数据，并都使用夏皮罗-威尔克检验严格地测试正态分布。下面来处理一些真实数据，并使用分位图（Q-Q plot）测试正态分布，正态分布的分位图看起来像一条平滑的对角线，而异常值可以通过这样的方法被直观地理解。但要注意分位图方法不像夏皮罗-威尔克检测那么正式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先载入相关库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:31.801140Z",
     "start_time": "2018-03-15T14:06:30.182573Z"
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从 Renthop 数据集中获取价格特征，并过滤最极端的值，使画出来的图比较清晰。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:31.801140Z",
     "start_time": "2018-03-15T14:06:30.182573Z"
    }
   },
   "outputs": [],
   "source": [
    "price = df.price[(df.price <= 20000) & (df.price > 500)]\n",
    "price_log = np.log(price)\n",
    "price_mm = MinMaxScaler().fit_transform(\n",
    "    price.values.reshape(-1, 1).astype(np.float64)).flatten()\n",
    "price_z = StandardScaler().fit_transform(\n",
    "    price.values.reshape(-1, 1).astype(np.float64)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制初始特征分位图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:32.717805Z",
     "start_time": "2018-03-15T14:06:31.802677Z"
    }
   },
   "outputs": [],
   "source": [
    "sm.qqplot(price, loc=price.mean(), scale=price.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对原数据应用标准缩放，画出标准缩放后的分位图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:32.987753Z",
     "start_time": "2018-03-15T14:06:32.719093Z"
    }
   },
   "outputs": [],
   "source": [
    "sm.qqplot(price_z, loc=price_z.mean(), scale=price_z.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图可见应用标准缩放后分位图形状并未改变。对原数据应用极小极大缩放，画出分位图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:33.243510Z",
     "start_time": "2018-03-15T14:06:32.988869Z"
    }
   },
   "outputs": [],
   "source": [
    "sm.qqplot(price_mm, loc=price_mm.mean(), scale=price_mm.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图可见，极小极大缩放后分位图形状依旧不变。对原数据取对数，画出分位图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:33.510998Z",
     "start_time": "2018-03-15T14:06:33.244652Z"
    }
   },
   "outputs": [],
   "source": [
    "sm.qqplot(price_log, loc=price_log.mean(), scale=price_log.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图可见对原数据取对数后，数据更接近正态分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 相互作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果前面的转换看起来更像是由数学驱动的，那这一小节更多地牵涉数据的本质。它既可以算特征转换，也可以算特征创建。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一个出租房屋的问题，使用的依旧是 Renthop 公司的数据集。该数据集的其中两个特征是「房间数」和「价格」。一般「每间房的价格」比「总价格」具有更多的指示信息，下面生成这一特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:33.800002Z",
     "start_time": "2018-03-15T14:06:33.512381Z"
    }
   },
   "outputs": [],
   "source": [
    "rooms = df[\"bedrooms\"].apply(lambda x: max(x, .5))\n",
    "df[\"price_per_bedroom\"] = df[\"price\"] / rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此过程中，要添加一些限制条件。因为如果将特征数限定在一定数目以下，就可以生成所有可能的相互作用，然后使用一些技术去除不必要的特征。此外，并非所有特征的相互作用都有实际意义；比如，线性模型中常用的多项式特征 [<i class=\"fa fa-external-link-square\" aria-hidden=\"true\"> sklearn.preprocessing.PolynomialFeatures</i>](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) 几乎无法被解释。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 缺失值补全"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很多算法无法处理缺失值，而真实数据常常都包含缺失值。幸运的是，Pandas 和 sklearn 都提供了易于使用的解决方案：`pandas.DataFrame.fillna` 和 `sklearn.preprocessing.Imputer`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些解决方案处理缺失值的方式很直接："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将缺失值编码为一个单独的空值，比如，类别变量使用 n/a 值；\n",
    "- 使用该特征最可能的值（数值变量使用均值或中位数，类别变量使用最常见的值）；\n",
    "- 或者，反其道而行之，使用某个极端值（比较适合决策树，因为这样使得决策树模型可以在缺失值和未缺失值间分割）；\n",
    "- 对于有序数据（列如时序数据），使用相邻值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有时库提供的解决方案会建议使用固定值，比如 df = df.fillna(0)，不再为缺失值浪费时间。然而，这并非最佳方案，因为数据预处理比创建模型花去的时间更多，因此简单的使用固定值可能会埋下隐患，损害模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么要进行特征选择？下面是两个重要的原因："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据越多，计算复杂度越高。而在真实的环境中，很容易碰上有数百个额外特征的数据。\n",
    "- 部分算法会将噪声（不含信息量的特征）视作信号，导致过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 统计学方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最先考虑移除的特征就是那些值不变的特征，即不包含信息的特征。沿着这一思路，那些方差较低的特征很可能不如方差较高的特征重要。所以，可以考虑移除方差低于特定阈值的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面实验一下，首先，生成具有 20 个特征的数据 x_data_generated。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:34.669290Z",
     "start_time": "2018-03-15T14:06:33.801760Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "x_data_generated, y_data_generated = make_classification()\n",
    "x_data_generated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "移除方差低于阈值（0.9）的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:34.674947Z",
     "start_time": "2018-03-15T14:06:34.670899Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "VarianceThreshold(.9).fit_transform(x_data_generated).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，移除了部分特征。下面对比一下原数据 x_data_generated 和移除部分特征后的数据 x_data_varth。首先载入相关库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:35.235212Z",
     "start_time": "2018-03-15T14:06:34.908459Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "移除方差低于阈值（0.9）的特征，得到 x_data_varth。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:35.235212Z",
     "start_time": "2018-03-15T14:06:34.908459Z"
    }
   },
   "outputs": [],
   "source": [
    "x_data_varth = VarianceThreshold(.9).fit_transform(x_data_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立逻辑回归模型，在这个模型上对比 x_data_generated 和 x_data_varth。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(solver='lbfgs', random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:35.251270Z",
     "start_time": "2018-03-15T14:06:35.237608Z"
    }
   },
   "outputs": [],
   "source": [
    "print(cross_val_score(logit, x_data_generated, y_data_generated,\n",
    "                      scoring='neg_log_loss', cv=5).mean())\n",
    "cross_val_score(logit, x_data_varth, y_data_generated,\n",
    "                scoring='neg_log_loss', cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面结果表明，选取特征提高了分类器的表现，虽然这个例子是使用的合成数据，但在真实问题上依旧值得使用特征选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 建模选取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征选择的另一种方法是使用某个基线模型评估特征，因为模型会表明特征的重要性。常用的两类模型是：随机森林和搭配 Lasso 正则的线性模型。这种方法的逻辑很简单：如果在简单模型中一些特征明显没用，那就没有必要将它们拖入更复杂的模型。下面实验一下这种方法，首先载入相关库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:35.975262Z",
     "start_time": "2018-03-15T14:06:35.502079Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合成一些数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:35.975262Z",
     "start_time": "2018-03-15T14:06:35.502079Z"
    }
   },
   "outputs": [],
   "source": [
    "x_data_generated, y_data_generated = make_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立基线模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:35.975262Z",
     "start_time": "2018-03-15T14:06:35.502079Z"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10, random_state=17)\n",
    "pipe = make_pipeline(SelectFromModel(estimator=rf), logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:35.975262Z",
     "start_time": "2018-03-15T14:06:35.502079Z"
    }
   },
   "outputs": [],
   "source": [
    "print(cross_val_score(logit, x_data_generated, y_data_generated,\n",
    "                      scoring='neg_log_loss', cv=5).mean())\n",
    "print(cross_val_score(rf, x_data_generated, y_data_generated,\n",
    "                      scoring='neg_log_loss', cv=5).mean())\n",
    "print(cross_val_score(pipe, x_data_generated, y_data_generated,\n",
    "                      scoring='neg_log_loss', cv=5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面结果表明此次建模选取特征改善了模型表现。当然，建模选取也有可能导致模型表现下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 网格搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，介绍最可靠但计算复杂度最高的方法：网格搜索。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在特征子集上训练模型，储存结果，在不同子集上重复这一过程，比较模型的质量以识别最佳特征集，这一方法被称为穷尽特征选取 [<i class=\"fa fa-external-link-square\" aria-hidden=\"true\"> Exhaustive Feature Selection</i>](http://rasbt.github.io/mlxtend/user_guide/feature_selection/ExhaustiveFeatureSelector/)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搜索所有组合通常会花去太长的时间，因此可以尝试缩减搜索空间。固定一个较小的数字 N，迭代所有 N 特征的组合，选定最佳组合，接着迭代 N+1 特征组合，其中，之前的最佳组合固定不变，仅仅考虑一个新特征。重复这些迭代过程，直到达到特征最大值，或者直到模型的表现停止明显的提升。这一算法被称为循序特征选取 [<i class=\"fa fa-external-link-square\" aria-hidden=\"true\"> Sequential Feature Selection</i>](http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面代码通过调用 `SequentialFeatureSelector()` 方法实现了循序特征选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T14:06:44.047841Z",
     "start_time": "2018-03-15T14:06:36.096849Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "selector = SequentialFeatureSelector(logit, scoring='neg_log_loss',\n",
    "                                     verbose=2, k_features=3, forward=False, n_jobs=-1)\n",
    "\n",
    "selector.fit(x_data_generated, y_data_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在机器学习领域，数据集的质量好坏直接决定了所建立的模型性能水平，因此学会数据预处理是十分重要的。本次实验介绍了特征提取、特征转换、特征选择的基本内容和常用方法，熟悉数据预处理的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fa fa-link\" aria-hidden=\"true\"> 相关链接</i>\n",
    "- [<i class=\"fa fa-external-link-square\" aria-hidden=\"true\"> 了解实验楼《楼+ 机器学习和数据挖掘课程》</i>](https://www.shiyanlou.com/louplus/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
